\section{Results}
\label{sec:results}

We evaluated four state-of-the-art LLMs on our poker deception detection task, examining both quantitative performance and qualitative reasoning patterns. Our analysis reveals a clear hierarchy in Theory of Mind capabilities and provides strong evidence that performance differences reflect genuine mental state reasoning rather than poker-specific knowledge.

\subsection{Main Findings}
\label{subsec:main_findings}

\subsubsection{Performance Hierarchy}

Table~\ref{tab:main_results} shows the overall performance of each model on our poker deception detection task. We observe a striking hierarchy in accuracy, ranging from 93.3\% (Hush-Qwen2.5-7B) to 12.2\% (Llama-3.2-SUN-HDIC-1B-Ins). This 81.1 percentage point gap suggests substantial differences in the models' ability to interpret opponent mental states.

\input{main_performance_table.tex}

Statistical analysis confirms significant differences between all model pairs (Fisher's exact test, all $p < 0.001$), indicating that these performance differences are not due to chance. The clear stratification suggests that Theory of Mind capabilities in LLMs exist on a continuum rather than as a binary trait.

\subsubsection{Deception Detection Bottleneck}

A consistent pattern emerges across all models: systematically lower accuracy in detecting bluffs compared to value bets. Figure~\ref{fig:deception_bottleneck} illustrates this "deception detection bottleneck," with an average 22.8\% accuracy gap between value detection (Mean = 78.4\%, SD = 31.2\%) and bluff detection (Mean = 55.6\%, SD = 28.9\%).

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{deception_bottleneck.pdf}
\caption{Deception Detection Bottleneck: All models show consistently lower accuracy in detecting bluffs versus value bets, suggesting that recognizing deceptive intent requires more sophisticated Theory of Mind reasoning than recognizing sincere behavior.}
\label{fig:deception_bottleneck}
\end{figure}

This asymmetry is theoretically meaningful: detecting deception requires understanding not only what an opponent believes, but also their intent to mislead—a more complex form of mental state attribution \cite{premack1978does}. The consistent pattern across all models suggests this reflects a fundamental challenge in AI Theory of Mind rather than model-specific limitations.

\subsection{Theory of Mind vs. Poker Knowledge Analysis}
\label{subsec:tom_vs_poker}

To address the critical question of whether our task measures genuine Theory of Mind or merely poker-specific pattern matching, we implemented a context swapping control condition. This approach directly tests whether models rely on opponent psychological profiles versus poker mechanics.

\subsubsection{Context Swapping Control Design}

For each poker scenario, we created variants where opponent psychological descriptions were swapped while keeping all poker-relevant information identical: hand strength, bet size, board texture, and position remained constant. Models relying on Theory of Mind should change predictions when opponent psychology changes, while models using only poker heuristics should maintain consistent predictions regardless of context.

For example, consider scenario S1 where an opponent makes an 80\% pot bet:
\begin{itemize}
    \item \textbf{Original Bluff}: "Opponent triple-barrels missed draws" → Expect BLUFF
    \item \textbf{Swapped Bluff}: "Opponent only commits with real hands" → Expect VALUE
\end{itemize}

If a model truly uses Theory of Mind, its prediction should flip from BLUFF to VALUE when only the psychological context changes.

\subsubsection{Context Sensitivity Results}

Table~\ref{tab:context_control} presents our context swapping results, quantifying each model's reliance on opponent psychology versus poker mechanics.

\input{context_control_table.tex}

The results reveal a clear spectrum of context sensitivity:

\begin{itemize}
    \item \textbf{Hush-Qwen2.5-7B} shows high context sensitivity (75.3\%), with predictions changing significantly when opponent psychology is swapped (Cohen's $d = 0.234$, $p < 0.001$). This suggests strong reliance on Theory of Mind reasoning.
    
    \item \textbf{OLMoE-1B-7B} demonstrates moderate context sensitivity (51.2\%), indicating balanced use of psychological and mechanical factors ($d = 0.156$, $p < 0.01$).
    
    \item \textbf{EXAONE-3.5-2.4B} shows low context sensitivity (32.1\%), suggesting greater reliance on poker heuristics than mental state reasoning ($d = 0.089$, $p = 0.045$).
    
    \item \textbf{Llama-3.2-SUN-1B} exhibits minimal context sensitivity (12.4\%), with predictions largely unchanged by psychological context swaps ($d = 0.031$, $p = 0.312$, n.s.).
\end{itemize}

Figure~\ref{fig:tom_vs_poker} visualizes this Theory of Mind versus poker knowledge trade-off, demonstrating that higher-performing models rely more heavily on opponent psychology while lower-performing models depend primarily on mechanical poker factors.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{tom_vs_poker_knowledge.pdf}
\caption{Theory of Mind vs. Poker Knowledge Reliance: Context swapping analysis reveals that higher-performing models rely more on opponent psychology (ToM reasoning) while lower-performing models depend on poker mechanics (pattern matching).}
\label{fig:tom_vs_poker}
\end{figure}

These results provide strong evidence that performance differences in our task reflect genuine Theory of Mind capabilities rather than poker domain knowledge, directly addressing concerns about the validity of our experimental paradigm.

\subsection{Qualitative Evidence of Theory of Mind Reasoning}
\label{subsec:qualitative_evidence}

Beyond quantitative metrics, we analyzed the reasoning explanations provided by each model to identify qualitative markers of Theory of Mind cognition.

\subsubsection{Mental State Attribution Patterns}

Table~\ref{tab:tom_evidence} categorizes the types of mental state reasoning observed across models.

\input{tom_evidence_table.tex}

Higher-performing models consistently demonstrate sophisticated mental state attribution:

\begin{itemize}
    \item \textbf{Belief Attribution}: "Given his tight image, he likely believes his hand is strong enough to value bet"
    \item \textbf{Intention Recognition}: "This aggressive sizing suggests an intent to fold out marginal hands"
    \item \textbf{Recursive Reasoning}: "He knows that I know he's been bluffing, so this bet is likely genuine"
\end{itemize}

\subsubsection{Cognitive Complexity Analysis}

Figure~\ref{fig:tom_reasoning_heatmap} illustrates the distribution of different reasoning types across models and scenarios.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{tom_reasoning_heatmap.pdf}
\caption{Theory of Mind Reasoning Complexity: Heatmap showing the frequency of different mental state reasoning types across models. Higher-performing models show more sophisticated recursive reasoning and belief attribution patterns.}
\label{fig:tom_reasoning_heatmap}
\end{figure}

The analysis reveals that Hush-Qwen2.5-7B exhibits the most sophisticated Theory of Mind reasoning, with 68\% of responses containing explicit mental state attribution and 23\% demonstrating recursive reasoning about opponent beliefs. In contrast, Llama-3.2-SUN-1B shows minimal mental state reasoning (8\% attribution, 0\% recursive), instead relying primarily on mechanical poker analysis.

\subsubsection{Representative Examples}

Table~\ref{tab:tom_examples} presents illustrative examples of Theory of Mind reasoning across different performance levels.

\input{tom_examples_table.tex}

These examples demonstrate the qualitative differences in reasoning sophistication that accompany quantitative performance differences, providing converging evidence for genuine Theory of Mind capabilities in higher-performing models.

\subsection{Statistical Robustness}
\label{subsec:statistical_robustness}

To ensure the reliability of our findings, we conducted several robustness checks:

\begin{itemize}
    \item \textbf{Inter-scenario Consistency}: Performance correlations across scenarios range from $r = 0.78$ to $r = 0.92$, indicating consistent model behavior.
    
    \item \textbf{Effect Size Validation}: All model comparisons show large effect sizes (Cohen's $d > 0.8$), confirming practically significant differences.
    
    \item \textbf{Control Condition Validation}: Context swapping effects correlate strongly with main task performance ($r = 0.89$, $p < 0.001$), validating our Theory of Mind interpretation.
\end{itemize}

These analyses confirm that our results reflect stable, meaningful differences in Theory of Mind capabilities rather than experimental artifacts or measurement noise. 