# üéØ Slide Content Text: Theory of Mind in LLMs

## **Slide 1: The Theory of Mind Challenge**
**Title**: Can LLMs Understand Opponent Psychology?

**Content**:
- **64.7%** of explanations show first-order Theory of Mind
- **22.7% gap** between value and bluff detection accuracy
- Deception detection as cognitive bottleneck

**One-line takeaway**: *"Testing whether LLMs can reason about opponent mental states in strategic contexts"*

---

## **Slide 2: Theoretical Framework**
**Title**: Theory of Mind Complexity Hierarchy

**Content**: *(Let visual tell the story - minimal text)*
- **Level 0**: Surface analysis only
- **Level 1**: "Opponent thinks X"  
- **Level 2**: "Opponent thinks I think Y"

**One-line takeaway**: *"Measuring sophistication of mental state reasoning"*

---

## **Slide 3: Sample Stimulus**
**Title**: Poker as Theory of Mind Test

**Content**: *(Visual-dominant slide)*
- Hero: A‚ô†K‚ô† | Board: Q‚ô†J‚ô†2‚ô•  
- Opponent bets $75 (50% pot)
- **Context matters**: "Opponent has shown aggressive bluffs tonight"

**Question**: *"BLUFF or VALUE?"*

---

## **Slide 4: Bluff Scenario**
**Title**: Deception Detection Challenge

**Content**:
- **Bluff-suggestive context**: "Opponent has bluffed 3 times tonight"
- **Model accuracy**: 50% (2/4 correct)
- Even sophisticated models struggle

**Key insight**: *"Detecting deception requires advanced ToM reasoning"*

---

## **Slide 5: Value Scenario**  
**Title**: Sincere Behavior Recognition

**Content**:
- **Value-suggestive context**: "Opponent plays tight, rarely bluffs"
- **Model accuracy**: 75% (3/4 correct)
- Clearer signal = better performance

**Key insight**: *"Recognizing sincere behavior is significantly easier"*

---

## **Slide 6: Experimental Design**
**Title**: Methodology Overview

**Content**: *(Visual-heavy, minimal text)*
- **4 models** √ó **15 scenarios** √ó **2 contexts** √ó **3 runs**
- **360 total responses** analyzed
- Temperature: 0.3 | Max tokens: 350

**Goal**: *"Systematic evaluation of ToM capabilities"*

---

## **Slide 7: Main Results**
**Title**: Cognitive Sophistication Drives Performance  

**Content**:
- **Hush-Qwen2.5-7B**: 93.3% accuracy (clear winner)
- **Strong correlation**: ToM level ‚Üî accuracy (r=0.89)
- Model scale matters for cognitive reasoning

**Key finding**: *"Larger models demonstrate superior Theory of Mind"*

---

## **Slide 8: Cognitive Analysis**
**Title**: Theory of Mind Patterns Revealed

**Content**: *(Dashboard speaks for itself)*
- **Context integration** strongest predictor (r=0.420)
- **Bluff detection** requires higher ToM levels
- **Model hierarchy** clearly evident

**Key insight**: *"Rich cognitive patterns emerge from strategic reasoning"*

---

## **Slide 9: The Deception Detection Bottleneck**
**Title**: Why Bluff Detection Is Harder

**Content**:
- **22.7% performance gap** (Value: 63.3% vs Bluff: 40.6%)
- **All models** struggle with deception
- Requires second-order mental modeling

**Key insight**: *"Understanding deceptive intent is AI's cognitive frontier"*

---

## **Slide 10: Theory of Mind Evidence**
**Title**: From Surface Analysis to Mental Modeling

**Content**: *(Split comparison visual)*

**Level 0 Example**: *"Board suggests flush, bet size indicates..."*
- ‚ùå No opponent psychology
- ‚ùå Cards/math focus only

**Level 2 Example**: *"Opponent is trying to exploit your range..."*  
- ‚úÖ Mental state attribution
- ‚úÖ Strategic deception reasoning

**Breakthrough**: *"Clear evidence of sophisticated ToM in LLM explanations"*

---

## **Slide 11: Research Implications**
**Title**: AI Theory of Mind: What We've Learned

**Content**: *(Circular diagram - minimal text)*
- **Measurable ToM** capabilities in LLMs
- **Deception detection** as cognitive bottleneck
- **Context integration** drives performance  
- **Model scale** affects ToM sophistication
- **Rich contexts** enable ToM reasoning

**Impact**: *"First systematic evidence of AI Theory of Mind in strategic reasoning"*

---

## **Slide 12: Future Directions**
**Title**: From Poker to Real-World Applications

**Content**:

**Current Findings** ‚Üí
- ToM capabilities measurable
- Deception detection challenging
- Context integration crucial

**Immediate Applications** ‚Üí
- AI safety alignment
- Social robot design
- Educational AI tutors

**Long-term Implications** ‚Üí
- Human-AI collaboration
- Social intelligence benchmarks
- Cognitive architecture insights

**Vision**: *"Advancing AI's understanding of human psychology"*

---

## üé§ **SPEAKER NOTES & TRANSITIONS**

### **Opening Hook** (Slide 1):
*"Today I'm going to show you evidence that large language models can demonstrate measurable Theory of Mind - the ability to understand what others are thinking. We tested this using poker scenarios..."*

### **Framework Transition** (Slide 2):
*"Theory of Mind exists on a spectrum of sophistication. Let me show you the hierarchy we used to evaluate LLM reasoning..."*

### **Methodology Hook** (Slide 3):
*"Here's what we showed the models. A poker scenario where context is everything..."*

### **Results Introduction** (Slide 7):
*"The results reveal a clear hierarchy of cognitive sophistication..."*

### **Deep Dive** (Slide 8):
*"When we analyzed the explanations through a cognitive lens, fascinating patterns emerged..."*

### **Key Discovery** (Slide 9):
*"Perhaps our most important finding: deception detection is significantly harder than recognizing sincere behavior..."*

### **Evidence** (Slide 10):
*"Let me show you concrete examples of the difference between surface analysis and genuine Theory of Mind reasoning..."*

### **Impact** (Slide 11):
*"These findings have broad implications for AI development..."*

### **Closing** (Slide 12):
*"This research opens new frontiers in understanding AI's social intelligence capabilities..."*

---

## üìù **KEY PRESENTATION PRINCIPLES**

1. **Minimal text** - let visuals carry the story
2. **One clear takeaway** per slide  
3. **Bold numbers** for impact (64.7%, 22.7%, 93.3%)
4. **Active language** - present tense, strong verbs
5. **Technical precision** - exact statistics, clear methodology
6. **Forward momentum** - each slide builds to the next

**Remember**: You have rich backup visualizations for any deep-dive questions during Q&A! 